{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "evaluation = True\n",
    "\n",
    "OUTPUT_BUCKET_FOLDER = \"gs://<GCS_BUCKET_NAME>/outbrain-click-prediction/output/\"\n",
    "DATA_BUCKET_FOLDER = \"gs://<GCS_BUCKET_NAME>/outbrain-click-prediction/data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "import pyspark.sql.functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import DataFrameWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import datetime\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "truncate_day_from_timestamp_udf = F.udf(lambda ts: int(ts / 1000 / 60 / 60 / 24), IntegerType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "extract_country_udf = F.udf(lambda geo: geo.strip()[:2] if geo != None else '', StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "documents_meta_schema = StructType(\n",
    "                    [StructField(\"document_id_doc\", IntegerType(), True),\n",
    "                    StructField(\"source_id\", IntegerType(), True),                    \n",
    "                    StructField(\"publisher_id\", IntegerType(), True),\n",
    "                    StructField(\"publish_time\", TimestampType(), True)]\n",
    "                    )\n",
    "\n",
    "documents_meta_df = spark.read.schema(documents_meta_schema).options(header='true', inferschema='false', nullValue='\\\\N') \\\n",
    "                .csv(DATA_BUCKET_FOLDER+\"documents_meta.csv\") \\\n",
    "                .withColumn('dummyDocumentsMeta', F.lit(1)).alias('documents_meta')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "documents_categories_schema = StructType(\n",
    "                    [StructField(\"document_id_cat\", IntegerType(), True),\n",
    "                    StructField(\"category_id\", IntegerType(), True),                    \n",
    "                    StructField(\"confidence_level_cat\", FloatType(), True)]\n",
    "                    )\n",
    "\n",
    "documents_categories_df = spark.read.schema(documents_categories_schema).options(header='true', inferschema='false', nullValue='\\\\N') \\\n",
    "                .csv(DATA_BUCKET_FOLDER+\"documents_categories.csv\") \\\n",
    "                .alias('documents_categories')\n",
    "    \n",
    "documents_categories_grouped_df = documents_categories_df.groupBy('document_id_cat') \\\n",
    "                                            .agg(F.collect_list('category_id').alias('category_id_list'),\n",
    "                                                 F.collect_list('confidence_level_cat').alias('cat_confidence_level_list')) \\\n",
    "                                            .withColumn('dummyDocumentsCategory', F.lit(1)) \\\n",
    "                                            .alias('documents_categories_grouped')                                          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "documents_topics_schema = StructType(\n",
    "                    [StructField(\"document_id_top\", IntegerType(), True),\n",
    "                    StructField(\"topic_id\", IntegerType(), True),                    \n",
    "                    StructField(\"confidence_level_top\", FloatType(), True)]\n",
    "                    )\n",
    "\n",
    "documents_topics_df = spark.read.schema(documents_topics_schema).options(header='true', inferschema='false', nullValue='\\\\N') \\\n",
    "                .csv(DATA_BUCKET_FOLDER+\"documents_topics.csv\")  \\\n",
    "                .alias('documents_topics')\n",
    "    \n",
    "documents_topics_grouped_df = documents_topics_df.groupBy('document_id_top') \\\n",
    "                                            .agg(F.collect_list('topic_id').alias('topic_id_list'),\n",
    "                                                 F.collect_list('confidence_level_top').alias('top_confidence_level_list')) \\\n",
    "                                            .withColumn('dummyDocumentsTopics', F.lit(1)) \\\n",
    "                                            .alias('documents_topics_grouped')                                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "documents_entities_schema = StructType(\n",
    "                    [StructField(\"document_id_ent\", IntegerType(), True),\n",
    "                    StructField(\"entity_id\", StringType(), True),                    \n",
    "                    StructField(\"confidence_level_ent\", FloatType(), True)]\n",
    "                    )\n",
    "\n",
    "documents_entities_df = spark.read.schema(documents_entities_schema).options(header='true', inferschema='false', nullValue='\\\\N') \\\n",
    "                .csv(DATA_BUCKET_FOLDER+\"documents_entities.csv\")  \\\n",
    "                .alias('documents_entities')\n",
    "    \n",
    "documents_entities_grouped_df = documents_entities_df.groupBy('document_id_ent') \\\n",
    "                                            .agg(F.collect_list('entity_id').alias('entity_id_list'),\n",
    "                                                 F.collect_list('confidence_level_ent').alias('ent_confidence_level_list')) \\\n",
    "                                            .withColumn('dummyDocumentsEntities', F.lit(1)) \\\n",
    "                                            .alias('documents_entities_grouped')                                          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "documents_df = documents_meta_df.join(documents_categories_grouped_df, on=F.col(\"document_id_doc\") == F.col(\"documents_categories_grouped.document_id_cat\"), how='left') \\\n",
    "                         .join(documents_topics_grouped_df, on=F.col(\"document_id_doc\") == F.col(\"documents_topics_grouped.document_id_top\"), how='left') \\\n",
    "                         .join(documents_entities_grouped_df, on=F.col(\"document_id_doc\") == F.col(\"documents_entities_grouped.document_id_ent\"), how='left') \\\n",
    "                         .cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2999334"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "if evaluation:\n",
    "    validation_set_df = spark.read.parquet(OUTPUT_BUCKET_FOLDER+\"validation_set.parquet\") \\\n",
    "                    .alias('validation_set')        \n",
    "    \n",
    "    validation_set_df.select('uuid_event').distinct().createOrReplaceTempView('users_to_profile')    \n",
    "    validation_set_df.select('uuid_event','document_id_promo').distinct().createOrReplaceTempView('validation_users_docs_to_ignore')\n",
    "    \n",
    "else:\n",
    "    events_schema = StructType(\n",
    "                    [StructField(\"display_id\", IntegerType(), True),\n",
    "                    StructField(\"uuid_event\", StringType(), True),                    \n",
    "                    StructField(\"document_id_event\", IntegerType(), True),\n",
    "                    StructField(\"timestamp_event\", IntegerType(), True),\n",
    "                    StructField(\"platform_event\", IntegerType(), True),\n",
    "                    StructField(\"geo_location_event\", StringType(), True)]\n",
    "                    )\n",
    "\n",
    "    events_df = spark.read.schema(events_schema).options(header='true', inferschema='false', nullValue='\\\\N') \\\n",
    "                    .csv(DATA_BUCKET_FOLDER+\"events.csv\") \\\n",
    "                    .withColumn('dummyEvents', F.lit(1)) \\\n",
    "                    .withColumn('day_event', truncate_day_from_timestamp_udf('timestamp_event')) \\\n",
    "                    .withColumn('event_country', extract_country_udf('geo_location_event')) \\\n",
    "                    .alias('events')\n",
    "\n",
    "    events_df.createOrReplaceTempView('events')\n",
    "\n",
    "\n",
    "    promoted_content_schema = StructType(\n",
    "                        [StructField(\"ad_id\", IntegerType(), True),\n",
    "                        StructField(\"document_id_promo\", IntegerType(), True),                    \n",
    "                        StructField(\"campaign_id\", IntegerType(), True),\n",
    "                        StructField(\"advertiser_id\", IntegerType(), True)]\n",
    "                        )\n",
    "\n",
    "    promoted_content_df = spark.read.schema(promoted_content_schema).options(header='true', inferschema='false', nullValue='\\\\N') \\\n",
    "                    .csv(DATA_BUCKET_FOLDER+\"promoted_content.csv\") \\\n",
    "                    .withColumn('dummyPromotedContent', F.lit(1)).alias('promoted_content')\n",
    "\n",
    "    clicks_test_schema = StructType(\n",
    "                        [StructField(\"display_id\", IntegerType(), True),\n",
    "                        StructField(\"ad_id\", IntegerType(), True)]\n",
    "                        )\n",
    "\n",
    "    clicks_test_df = spark.read.schema(clicks_test_schema).options(header='true', inferschema='false', nullValue='\\\\N') \\\n",
    "                    .csv(DATA_BUCKET_FOLDER+\"clicks_test.csv\") \\\n",
    "                    .withColumn('dummyClicksTest', F.lit(1)).alias('clicks_test')\n",
    "    \n",
    "    test_set_df = clicks_test_df.join(promoted_content_df, on='ad_id', how='left') \\\n",
    "                                .join(events_df, on='display_id', how='left')\n",
    "        \n",
    "    test_set_df.select('uuid_event').distinct().createOrReplaceTempView('users_to_profile')\n",
    "    test_set_df.select('uuid_event','document_id_promo', 'timestamp_event').distinct().createOrReplaceTempView('test_users_docs_timestamp_to_ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "page_views_schema = StructType(\n",
    "                    [StructField(\"uuid_pv\", StringType(), True),\n",
    "                    StructField(\"document_id_pv\", IntegerType(), True),\n",
    "                    StructField(\"timestamp_pv\", IntegerType(), True),\n",
    "                    StructField(\"platform_pv\", IntegerType(), True),\n",
    "                    StructField(\"geo_location_pv\", StringType(), True),\n",
    "                    StructField(\"traffic_source_pv\", IntegerType(), True)]\n",
    "                    )\n",
    "page_views_df = spark.read.schema(page_views_schema).options(header='true', inferschema='false', nullValue='\\\\N') \\\n",
    "                .csv(DATA_BUCKET_FOLDER+\"page_views.csv\") \\\n",
    "                .alias('page_views')        \n",
    "            \n",
    "page_views_df.createOrReplaceTempView('page_views')      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "additional_filter = ''\n",
    "if evaluation:\n",
    "    additional_filter = '''\n",
    "                             AND NOT EXISTS (SELECT uuid_event FROM validation_users_docs_to_ignore \n",
    "                                                      WHERE uuid_event = p.uuid_pv\n",
    "                                                     AND document_id_promo = p.document_id_pv)\n",
    "                        '''\n",
    "else:\n",
    "    additional_filter = '''\n",
    "                             AND NOT EXISTS (SELECT uuid_event FROM test_users_docs_timestamp_to_ignore \n",
    "                                                      WHERE uuid_event = p.uuid_pv\n",
    "                                                     AND document_id_promo = p.document_id_pv\n",
    "                                                     AND p.timestamp_pv >= timestamp_event)\n",
    "                        '''\n",
    "\n",
    "page_views_train_df = spark.sql('''SELECT * FROM page_views p \n",
    "                                    WHERE EXISTS (SELECT uuid_event FROM users_to_profile\n",
    "                                                 WHERE uuid_event = p.uuid_pv)                                     \n",
    "                                '''+ additional_filter\n",
    "                               ).alias('views') \\\n",
    "                         .join(documents_df, on=F.col(\"document_id_pv\") == F.col(\"document_id_doc\"), how='left') \\\n",
    "                         .filter('dummyDocumentsEntities is not null OR dummyDocumentsTopics is not null OR dummyDocumentsCategory is not null')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Processing document frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2999334"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents_total = documents_meta_df.count()\n",
    "documents_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "97"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categories_docs_counts = documents_categories_df.groupBy('category_id').count().rdd.collectAsMap()\n",
    "len(categories_docs_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_filenames_suffix = ''\n",
    "if evaluation:\n",
    "    df_filenames_suffix = '_eval'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('categories_docs_counts'+df_filenames_suffix+'.pickle', 'wb') as output:\n",
    "    pickle.dump(categories_docs_counts, output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topics_docs_counts = documents_topics_df.groupBy('topic_id').count().rdd.collectAsMap()\n",
    "len(topics_docs_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('topics_docs_counts'+df_filenames_suffix+'.pickle', 'wb') as output:\n",
    "    pickle.dump(topics_docs_counts, output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1326009"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entities_docs_counts = documents_entities_df.groupBy('entity_id').count().rdd.collectAsMap()\n",
    "len(entities_docs_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('entities_docs_counts'+df_filenames_suffix+'.pickle', 'wb') as output:\n",
    "    pickle.dump(entities_docs_counts, output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing user profiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "int_null_to_minus_one_udf = F.udf(lambda x: x if x != None else -1, IntegerType())\n",
    "int_list_null_to_empty_list_udf = F.udf(lambda x: x if x != None else [], ArrayType(IntegerType()))\n",
    "float_list_null_to_empty_list_udf = F.udf(lambda x: x if x != None else [], ArrayType(FloatType()))\n",
    "str_list_null_to_empty_list_udf = F.udf(lambda x: x if x != None else [], ArrayType(StringType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "page_views_by_user_df = page_views_train_df.select(\n",
    "                           'uuid_pv', \n",
    "                           'document_id_pv', \n",
    "                           int_null_to_minus_one_udf('timestamp_pv').alias('timestamp_pv'), \n",
    "                           int_list_null_to_empty_list_udf('category_id_list').alias('category_id_list'), \n",
    "                           float_list_null_to_empty_list_udf('cat_confidence_level_list').alias('cat_confidence_level_list'), \n",
    "                           int_list_null_to_empty_list_udf('topic_id_list').alias('topic_id_list'), \n",
    "                           float_list_null_to_empty_list_udf('top_confidence_level_list').alias('top_confidence_level_list'), \n",
    "                           str_list_null_to_empty_list_udf('entity_id_list').alias('entity_id_list'), \n",
    "                           float_list_null_to_empty_list_udf('ent_confidence_level_list').alias('ent_confidence_level_list')) \\\n",
    "                    .groupBy('uuid_pv') \\\n",
    "                    .agg(F.collect_list('document_id_pv').alias('document_id_pv_list'),\n",
    "                         F.collect_list('timestamp_pv').alias('timestamp_pv_list'),\n",
    "                         F.collect_list('category_id_list').alias('category_id_lists'),\n",
    "                         F.collect_list('cat_confidence_level_list').alias('cat_confidence_level_lists'),\n",
    "                         F.collect_list('topic_id_list').alias('topic_id_lists'),\n",
    "                         F.collect_list('top_confidence_level_list').alias('top_confidence_level_lists'),\n",
    "                         F.collect_list('entity_id_list').alias('entity_id_lists'),\n",
    "                         F.collect_list('ent_confidence_level_list').alias('ent_confidence_level_lists')\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def get_user_aspects(docs_aspects, aspect_docs_counts):\n",
    "    docs_aspects_merged_lists = defaultdict(list)\n",
    "    \n",
    "    for doc_aspects in docs_aspects:\n",
    "        for key in doc_aspects.keys():\n",
    "            docs_aspects_merged_lists[key].append(doc_aspects[key])\n",
    "        \n",
    "    docs_aspects_stats = {}\n",
    "    for key in docs_aspects_merged_lists.keys():\n",
    "        aspect_list = docs_aspects_merged_lists[key]\n",
    "        tf = len(aspect_list)\n",
    "        idf = math.log(documents_total / float(aspect_docs_counts[key]))\n",
    "        \n",
    "        confid_mean = sum(aspect_list) / float(len(aspect_list))\n",
    "        docs_aspects_stats[key] = [tf*idf, confid_mean]\n",
    "\n",
    "        \n",
    "    return docs_aspects_stats\n",
    "\n",
    "\n",
    "def generate_user_profile(docs_aspects_list, docs_aspects_confidence_list, aspect_docs_counts):    \n",
    "    docs_aspects = []\n",
    "    for doc_aspects_list, doc_aspects_confidence_list in zip(docs_aspects_list, docs_aspects_confidence_list):\n",
    "        doc_aspects = dict(zip(doc_aspects_list, doc_aspects_confidence_list))\n",
    "        docs_aspects.append(doc_aspects)\n",
    "        \n",
    "    user_aspects = get_user_aspects(docs_aspects, aspect_docs_counts)\n",
    "    return user_aspects\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "get_list_len_udf = F.udf(lambda docs_list: len(docs_list), IntegerType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "generate_categories_user_profile_map_udf = F.udf(lambda docs_aspects_list, \n",
    "                                                 docs_aspects_confidence_list: \\\n",
    "                                                      generate_user_profile(docs_aspects_list, \n",
    "                                                                            docs_aspects_confidence_list, \n",
    "                                                                            categories_docs_counts), \n",
    "                                          MapType(IntegerType(), \n",
    "                                                  ArrayType(FloatType()),\n",
    "                                                  False))\n",
    "\n",
    "\n",
    "generate_topics_user_profile_map_udf = F.udf(lambda docs_aspects_list, \n",
    "                                                 docs_aspects_confidence_list: \\\n",
    "                                                      generate_user_profile(docs_aspects_list, \n",
    "                                                                            docs_aspects_confidence_list, \n",
    "                                                                            topics_docs_counts), \n",
    "                                          MapType(IntegerType(), \n",
    "                                                  ArrayType(FloatType()),\n",
    "                                                  False))\n",
    "\n",
    "\n",
    "generate_entities_user_profile_map_udf = F.udf(lambda docs_aspects_list, \n",
    "                                                 docs_aspects_confidence_list: \\\n",
    "                                                      generate_user_profile(docs_aspects_list, \n",
    "                                                                            docs_aspects_confidence_list, \n",
    "                                                                            entities_docs_counts), \n",
    "                                          MapType(StringType(),\n",
    "                                                  ArrayType(FloatType()),\n",
    "                                                  False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "users_profile_df = page_views_by_user_df \\\n",
    "                                 .withColumn('views', get_list_len_udf('document_id_pv_list')) \\\n",
    "                                 .withColumn('categories', \n",
    "                                             generate_categories_user_profile_map_udf('category_id_lists', \n",
    "                                                                   'cat_confidence_level_lists')) \\\n",
    "                                 .withColumn('topics', \n",
    "                                             generate_topics_user_profile_map_udf('topic_id_lists', \n",
    "                                                                               'top_confidence_level_lists')) \\\n",
    "                                 .withColumn('entities', \n",
    "                                             generate_entities_user_profile_map_udf('entity_id_lists', \n",
    "                                                                               'ent_confidence_level_lists')) \\\n",
    "                                 .select(F.col('uuid_pv').alias('uuid'),\n",
    "                                         F.col('document_id_pv_list').alias('doc_ids'),\n",
    "                                         'views',\n",
    "                                         'categories', 'topics', 'entities')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if evaluation:\n",
    "    table_name = 'user_profiles_eval'\n",
    "else:\n",
    "    table_name = 'user_profiles'\n",
    "\n",
    "\n",
    "users_profile_df.write.parquet(OUTPUT_BUCKET_FOLDER+table_name, mode='overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Elapsed min: ', 0.934258288608657)\n"
     ]
    }
   ],
   "source": [
    "finish_time = time.time()\n",
    "print(\"Elapsed min: \", (finish_time-start_time)/60/60)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
